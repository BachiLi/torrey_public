\input{preamble}

\begin{document}

\header{2}{Triangles and acceleration structures}

In this homework, we are going to make your renderer capable of rendering more complex scenes.
The most commonly used primitive in computer graphics is a triangle. We will extend your renderer to handle objects that are made of \emph{triangle meshes}. Looping over each triangle is very slow. A scene can easily contain millions or billions of triangles (for example, in the movie Moana, \href{https://www.disneyanimation.com/resources/moana-island-scene/}{a scene of its island} contains more than 15 billion primitives). We will need something better.

For this homework, we will move to the second book of the Ray Tracing in One Weekend series: \href{https://raytracing.github.io/books/RayTracingTheNextWeek.html}{Ray Tracing - The Next Week (RTNW)}. As usual, we take huge inspirations from Wojciech Jarosz's \href{https://cs87-dartmouth.github.io/Fall2022/assignments.html}{CS 87/287 at Dartmouth}.

\section{Intersection with a single triangle}
Like our previous homework, let's start with rendering a single triangle. There are many ways to do it with different degrees of efficiency. The RTNW book did not provide information about triangles, but you can easily find some online. The \href{https://en.wikipedia.org/wiki/M%C3%B6ller%E2%80%93Trumbore_intersection_algorithm}{Moller-Trumbore intersection algorithm} described in the Wikipedia page is useful. The \href{https://www.pbr-book.org/3ed-2018/Shapes/Triangle_Meshes#TriangleIntersection}{PBRT book} contains more details.
Or you can just google ray-triangle intersection.
Feel free to copy paste any code.

Go to \lstinline{hw2.cpp} and look at the function \lstinline{hw_2_1}. Using the camera parameters provided (\lstinline{lookfrom = (0, 0, 0), lookat = (0, 0, -1), up = (0, 1, 0), vfov = 45}), render a triangle read from the command line arguments (we have parsed them for you). If your ray hits a triangle, outputs its barycentric coordinates (for a triangle $p_0, p_1, p_2$, a point inside the triangle can be represented as $p = w p_0 + u p_1 + v p_2$, where $(w, u, v)$ is the barycentric coordinate and $w = 1 - u - v$). Otherwise, if the ray does not hit anything, return $(0.5, 0.5, 0.5)$. To have consistency with our homework 1 code, let's do antialiasing. We have parsed the \lstinline{-spp} argument for you too.

To see your results, type the following in the terminal:
\begin{lstlisting}[language=bash]
  ./torrey -hw 2_1 0  1 -3  -1 -1 -3  1 -1 -3
  ./torrey -hw 2_1 0  3 -4  -1 -2 -5  5 -1 -3
  ./torrey -hw 2_1 0 -3  1  -1  1 -4  1  1 -4
\end{lstlisting}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.3\linewidth]{imgs/hw_2_1a.png}
    \includegraphics[width=0.3\linewidth]{imgs/hw_2_1b.png}
    \includegraphics[width=0.3\linewidth]{imgs/hw_2_1c.png}
    \caption{References for homework 2.1.}
    \label{fig:hw_2_1}
\end{figure}

\section{Intersection with a triangle mesh}
Let's extend the previous code to handle multiple triangles now. In computer graphics, we usually organize triangles in a \href{https://en.wikipedia.org/wiki/Triangle_mesh}{triangle mesh}. A triangle mesh consists of a list of \emph{vertices} which is just a bunch of 3D points, and a list of \emph{indices}, which connect the vertices into triangles. For example, following code represents a tetrahedron:
\begin{lstlisting}[language=C++]
    std::vector<Vector3> positions = {
        Vector3{ 0.0,  0.5, -2.0},
        Vector3{ 0.0, -0.3, -1.0},
        Vector3{ 1.0, -0.5, -3.0},
        Vector3{-1.0, -0.5, -3.0}
    };
    std::vector<Vector3i> indices = {
        Vector3i{0, 1, 2},
        Vector3i{0, 3, 1},
        Vector3i{0, 2, 3},
        Vector3i{1, 2, 3}
    };
\end{lstlisting}

Go to function \lstinline{hw_2_2} and extend your previous code to render the tetrahedron above. We will use the same camera pose, and we will again output the barycentric coordinates of the triangles. The \lstinline{-spp} argument is still there, but we don't need to touch it.

To see your results, type the following in the terminal:
\begin{lstlisting}[language=bash]
  ./torrey -hw 2_2
\end{lstlisting}

You should render something like Figure~\ref{fig:hw_2_2}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\linewidth]{imgs/hw_2_2.png}
    \caption{References for homework 2.2.}
    \label{fig:hw_2_2}
\end{figure}

\section{Parsing and rendering a scene.}
So far, we specify the scenes by either hardcoding them in code or we input a bunch of numbers in the command line. 
This is inconvienent. Usually, the input to a 3D rendering software is a text file describing a scene. It specifies the camera pose, lights, the materials of the objects, the geometry, and the rendering settings. There are many existing scene formats. For example see the \href{https://en.wikipedia.org/wiki/RenderMan_Interface_Specification}{RenderMan interface} and Pixar's \href{https://en.wikipedia.org/wiki/Universal_Scene_Description}{Universal Scene Description}. Here, we adopt the scene format from the \href{https://mitsuba.readthedocs.io/}{Mitsuba renderer}, simply because the scene file format is quite readable and there is an existing \href{https://github.com/mitsuba-renderer/mitsuba-blender}{Blender exporter}. We have provided the scene file parser for you, and you just need to convert the parsed information into your own data structure.

A typical Mitsuba scene is an XML file that looks like the following:
\begin{lstlisting}[language=xml]
<?xml version="1.0" encoding="utf-8"?>
<scene version="0.6.0">
  <sensor type="perspective">
    <string name="fovAxis" value="y"/>
    <transform name="toWorld">
      <lookAt origin="278, 273, -800" target="278, 273, -799" up="0, 1, 0"/>
    </transform>
    <float name="fov" value="39.3077"/>
    <sampler type="independent">
      <integer name="sampleCount" value="16"/>
    </sampler>
    <film type="hdrfilm">
      <integer name="width" value="512"/>
      <integer name="height" value="512"/>
    </film>
  </sensor>
  <!-- materials -->
  <bsdf type="diffuse" id="box">
    <rgb name="reflectance" value="0.884774, 0.699933, 0.666224"/>
  </bsdf>
  <bsdf type="diffuse" id="white">
    <rgb name="reflectance" value="0.884774, 0.699933, 0.666224"/>
  </bsdf>
  <bsdf type="diffuse" id="red">
    <rgb name="reflectance" value="0.56581, 0.0447145, 0.0441583"/>
  </bsdf>
  <bsdf type="diffuse" id="green">
    <rgb name="reflectance" value="0.105092, 0.378697, 0.0762035"/>
  </bsdf>
  <!-- lights -->
  <emitter type="point">
        <rgb name="intensity" value="1, 1, 1"/>
        <point name="position" x="278" y="548.8" z="279.5"/>
  </emitter>
  <!-- shapes -->
  <shape type="obj">
    <string name="filename" value="meshes/cbox_floor.obj"/>
    <ref id="white"/>
  </shape>
  <shape type="obj">
    <string name="filename" value="meshes/cbox_ceiling.obj"/>
    <ref id="white"/>
  </shape>
  <shape type="obj">
    <string name="filename" value="meshes/cbox_back.obj"/>
    <ref id="white"/>
  </shape>
  <shape type="obj">
    <string name="filename" value="meshes/cbox_greenwall.obj"/>
    <ref id="green"/>
  </shape>
  <shape type="obj">
    <string name="filename" value="meshes/cbox_redwall.obj"/>
    <ref id="red"/>
  </shape>
  <shape type="obj">
    <string name="filename" value="meshes/cbox_smallbox.obj"/>
    <ref id="box"/>
  </shape>
  <shape type="obj">
    <string name="filename" value="meshes/cbox_largebox.obj"/>
    <ref id="box"/>
  </shape>
</scene>
\end{lstlisting}

The whole scene is contained in a \lstinline{<scene>} tag. 
The scene is consists of \lstinline{sensor} (camera), \lstinline{bsdf} (materials), \lstinline{emitter} (lights), and \lstinline{shape} (geometry). 

The camera is specified as follows:
\begin{lstlisting}[language=xml]
  <sensor type="perspective">
    <string name="fovAxis" value="y"/>
    <transform name="toWorld">
      <lookAt origin="278, 273, -800" target="278, 273, -799" up="0, 1, 0"/>
    </transform>
    <float name="fov" value="39.3077"/>
    <sampler type="independent">
      <integer name="sampleCount" value="16"/>
    </sampler>
    <film type="hdrfilm">
      <integer name="width" value="512"/>
      <integer name="height" value="512"/>
    </film>
  </sensor>
\end{lstlisting}
We only support perspective cameras, and the \lstinline{transform} inside a sensor can only be a \lstinline{lookAt} transform. We only support \emph{independent} samplers, and inside it we specify the number of samples per pixel. We only support \lstinline{hdrfilm} which outputs an exr image with the specified width and height.

We can declare a list of materials and assign them to objects. A declaration looks like this:
\begin{lstlisting}[language=xml]
  <bsdf type="diffuse" id="box">
    <rgb name="reflectance" value="0.884774, 0.699933, 0.666224"/>
  </bsdf>
\end{lstlisting}
For this homework, you only need to support \lstinline{diffuse} and \lstinline{mirror} materials (just like the previous homework). In the future homeworks we will need to support more.

We can also declare a list of lights:
\begin{lstlisting}[language=xml]
  <emitter type="point">
        <rgb name="intensity" value="1, 1, 1"/>
        <point name="position" x="278" y="548.8" z="279.5"/>
  </emitter>
\end{lstlisting}
For this homework, you only need to support \lstinline{point} lights. In the future we will implement more lights.

Finally, we declare a list of shapes:
\begin{lstlisting}[language=xml]
  <shape type="obj">
    <string name="filename" value="meshes/cbox_floor.obj"/>
    <ref id="white"/>
  </shape>
\end{lstlisting}
A shape can be an \lstinline{obj} file, a \lstinline{ply} file, or a sphere. An \href{https://en.wikipedia.org/wiki/Wavefront_.obj_file}{obj file} specifies a triangle mesh and looks like this:
\begin{lstlisting}
v 0 0 559.20001
v 0 0 0
v 0 548.79999 0
v 0 548.79999 559.20001
f 1 2 3 4
\end{lstlisting}
It specifies the 3D positions of vertices and the indices.

A \lstinline{ply} file is similar but stores the information in binary formats, making it more compact and efficient. Read the \href{https://en.wikipedia.org/wiki/PLY_(file_format)}{Wikipedia page} for more information.

A shape can also be a sphere:
\begin{lstlisting}[language=xml]
  <shape type="sphere">
    <point name="center" x="1" y="2" z="3"/>
    <float name="radius" value="10"/>
    <ref id="white"/>
  </shape>
\end{lstlisting}

The \lstinline{<ref>} tag references the materials we declared earlier and assign it to the shape. We can also declare the material inline as the following:
\begin{lstlisting}[language=xml]
  <shape type="sphere">
    <point name="center" x="1" y="2" z="3"/>
    <float name="radius" value="10"/>
    <bsdf type="diffuse">
      <rgb name="reflectance" value="0.5, 0.5, 0.5"/>
    </bsdf>
  </shape>
\end{lstlisting}

Finally, we can apply \emph{transformations} to shapes. The transformations supported are \lstinline{scale}, \lstinline{translate}, \lstinline{rotate}, \lstinline{lookat}, and \lstinline{matrix}. You specify it inside the shape:
\begin{lstlisting}[language=xml]
  <shape type="obj">
    <transform name="toWorld">
      <scale x="2" y="2" z="2"/>
      <translate x="5" y="6" z="7"/>
      <rotate angle="60" x="1" y="0" z="0"/>
      <matrix value="0 -0.53 0 -1.79 0.92 0 0 8.03 0 0 0.53 0 0 0 0 1"/>
    </transform>
    <string name="filename" value="meshes/cbox_floor.obj"/>
    <ref id="white"/>
  </shape>
\end{lstlisting}
We will apply the transformations in order. The matrix transform specifies the transformation in row-major order.
The transformation is only supported with \lstinline{obj} and \lstinline{ply}. You can't transform a sphere (we just want to make things simpler -- it is actually easy to implement transformed spheres).

After passing the xml file into our parser (using the \lstinline{parse_scene} function defined in \lstinline{parse_scene.h} and \lstinline{parse_scene.cpp}), it returns a \lstinline{ParsedScene}:
\begin{lstlisting}[language=C++]
struct ParsedScene {
    ParsedCamera camera;
    std::vector<ParsedMaterial> materials;
    std::vector<ParsedLight> lights;
    std::vector<ParsedShape> shapes;
    int samples_per_pixel;
};
\end{lstlisting}
where \lstinline{ParsedCamera} is defined as:
\begin{lstlisting}[language=C++]
struct ParsedCamera {
    Vector3 lookfrom;
    Vector3 lookat;
    Vector3 up;
    Real vfov;
    int width, height;
};
\end{lstlisting}
\lstinline{ParsedMaterial} is defined as:
\begin{lstlisting}[language=C++]
struct ParsedDiffuse {
    ParsedColor reflectance;
};
struct ParsedMirror {
    ParsedColor reflectance;
};
using ParsedMaterial = std::variant<ParsedDiffuse, ParsedMirror>;
\end{lstlisting}
\lstinline{ParsedLight} is defined as:
\begin{lstlisting}[language=C++]
struct ParsedPointLight {
    Vector3 position;
    Vector3 intensity;
};
// ...
using ParsedLight = std::variant<ParsedPointLight, ParsedDiffuseAreaLight>;
\end{lstlisting}
\lstinline{ParsedShape} is defined as:
\begin{lstlisting}[language=C++]
struct ParsedShapeBase {
    int material_id = -1;
    int area_light_id = -1;
};
struct ParsedSphere : public ParsedShapeBase {
    Vector3 position;
    Real radius;
};
struct ParsedTriangleMesh : public ParsedShapeBase {
    std::vector<Vector3> positions;
    std::vector<Vector3i> indices;
    std::vector<Vector3> normals;
    std::vector<Vector2> uvs;
};
using ParsedShape = std::variant<ParsedSphere, ParsedTriangleMesh>;
\end{lstlisting}
and \lstinline{ParsedColor} is defined as:
\begin{lstlisting}[language=C++]
struct ParsedImageTexture {
    fs::path filename;
    Real uscale = 1, vscale = 1;
    Real uoffset = 0, voffset = 0;
};
using ParsedColor = std::variant<Vector3 /* RGB */, ParsedImageTexture>;
\end{lstlisting}

There is a bit of information to unpack. \lstinline{ParsedCamera} should be straightforward.
\lstinline{ParsedMaterial}, \lstinline{ParsedLight}, and \lstinline{ParsedShape} are defined using 
C++17 feature \lstinline{std::variant}. It's basically a \emph{tagged union} that encapsulates:
\begin{lstlisting}[language=C++]
struct Foo {
  int type;
  union {
    Type0 t0;
    Type1 t1;
    // ...
  }
};
\end{lstlisting}
Read more about it \href{https://www.cppstories.com/2020/04/variant-virtual-polymorphism.html/}{here}.

You do not need to worry about area lights.
You do not need to worry about \lstinline{normals} and \lstinline{uvs} in \lstinline{ParsedTriangleMesh}.
You do not need to worry about \lstinline{ParsedImageTexture}.
We will implement all of these in Homework 3.

What you need to do is to render the scenes we provide using your renderer in the function \lstinline{hw_2_3}. This should mostly just be converting the parsed data structure into your own data structures. 
We have provided scenes that reproduce the ones you rendered in homework 1. 
We also provided a few new ones that contains triangle meshes.

This part of the homework is a bit tedious and slightly involved. We provide here the constructor of our own scene data structure that converts a \lstinline{ParsedScene} into our scene. You do not have to follow this:
\begin{lstlisting}[language=C++]
struct Scene {
    Scene(const ParsedScene &scene);

    Camera camera;
    int width, height;
    std::vector<Shape> shapes;
    std::vector<Material> materials;
    std::vector<Light> lights;

    int samples_per_pixel;
    // For the Triangle in the shapes to reference to.
    std::vector<TriangleMesh> meshes;
};

Scene::Scene(const ParsedScene &scene) :
        camera(from_parsed_camera(scene.camera)),
        width(scene.camera.width),
        height(scene.camera.height),
        samples_per_pixel(scene.samples_per_pixel) {
    // Extract triangle meshes from the parsed scene.
    int tri_mesh_count = 0;
    for (const ParsedShape &parsed_shape : scene.shapes) {
        if (std::get_if<ParsedTriangleMesh>(&parsed_shape)) {
            tri_mesh_count++;
        }
    }
    meshes.resize(tri_mesh_count);

    // Extract the shapes
    tri_mesh_count = 0;
    for (int i = 0; i < (int)scene.shapes.size(); i++) {
        const ParsedShape &parsed_shape = scene.shapes[i];
        if (auto *sph = std::get_if<ParsedSphere>(&parsed_shape)) {
            shapes.push_back(
                Sphere{{sph->material_id, sph->area_light_id},
                    sph->position, sph->radius});
        } else if (auto *parsed_mesh = std::get_if<ParsedTriangleMesh>(&parsed_shape)) {
            meshes[tri_mesh_count] = TriangleMesh{
                {parsed_mesh->material_id, parsed_mesh->area_light_id},
                parsed_mesh->positions, parsed_mesh->indices,
                parsed_mesh->normals, parsed_mesh->uvs};
            // Extract all the individual triangles
            for (int face_index = 0; face_index < (int)parsed_mesh->indices.size(); face_index++) {
                shapes.push_back(Triangle{face_index, &meshes[tri_mesh_count]});
            }
            tri_mesh_count++;
        } else {
            assert(false);
        }
    }

    // Copy the materials
    for (const ParsedMaterial &parsed_mat : scene.materials) {
        if (auto *diffuse = std::get_if<ParsedDiffuse>(&parsed_mat)) {
            // We assume the reflectance is always RGB for now.
            materials.push_back(Diffuse{std::get<Vector3>(diffuse->reflectance)});
        } else if (auto *mirror = std::get_if<ParsedMirror>(&parsed_mat)) {
            // We assume the reflectance is always RGB for now.
            materials.push_back(Mirror{std::get<Vector3>(mirror->reflectance)});
        }
    }

    // Copy the lights
    for (const ParsedLight &parsed_light : scene.lights) {
        // We assume the light is point light for now.
        ParsedPointLight point_light = std::get<ParsedPointLight>(parsed_light);
        lights.push_back(PointLight{point_light.intensity, point_light.position});
    }
}
\end{lstlisting}
Note that we convert a triangle mesh into many triangle primitives. This will make the later part of the homework much easier and we recommend you to do so too:
\begin{lstlisting}[language=C++]
struct Triangle {
    int face_index;
    const TriangleMesh *mesh;
};
\end{lstlisting}

To see your results, type the following in the command line:
\begin{lstlisting}[language=bash]
./torrey -hw 2_3 [scene_filename]
\end{lstlisting}
For example, if you are in the build directory, \lstinline{./torrey -hw 2_3 ../scenes/hw1_spheres/scene0.xml} would render a scene from Homework 1.

Test your code in the following scenes: 
\begin{itemize}
  \item \lstinline{scenes/hw1_spheres/scene0.xml} to \lstinline{scenes/hw1_spheres/scene4.xml} are the scenes from the previous homework, and you should get the exact same results. 
  \item \lstinline{scenes/hw2_triangles/single_triangle.xml} and \lstinline{scenes/hw2_triangles/tetrahedron.xml} are the two scenes above.
  \item \lstinline{scenes/cbox_point_light/cbox.xml} and \lstinline{scenes/cbox_point_light_spheres/cbox.xml} renders into images in Figure~\ref{fig:hw_2_3}.
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.3\linewidth]{imgs/hw_2_3a.png}
    \includegraphics[width=0.3\linewidth]{imgs/hw_2_3b.png}
    \caption{References for homework 2.3.}
    \label{fig:hw_2_3}
\end{figure}

\section{Intersection with axis-aligned bounding boxes}
So far we have only rendered scenes with relatively small amount of primitives. It would be cool if we can render more complex scenes, but our current way to handle ray-scene intersection would not scale to a high number of primitives. To improve this, we will implement an \emph{acceleration data structure}. The one we will implement in this homework is the \emph{bounding volume hierarchy} -- it's the most popular one in  modern ray tracers. We will use axis-aligned bounding boxes as our bounding volumes -- they are shown to be very fast to intersect with and provide a relatively tight bounds for our primitives. Thus, a crucial component of a typical bounding volume hierarchy is the intersection between rays and axis aligned bounding boxes. 

In this part, we will implement the intersection routine between a ray and an axis-aligned bounding box. First, read \href{https://raytracing.github.io/books/RayTracingTheNextWeek.html#boundingvolumehierarchies}{Chapters 3.1 to 3.7} of RTNW and understand the content.

Next, go to function \lstinline{hw_2_4} and modify the code. Your code should take a scene as an input just like in homework 2.3. You will construct an axis-aligned bounding box for each sphere and each triangle. You will next intersect the camera rays against these boxes. If the ray hits any of the boxes, outputs the color $(1, 1, 1)$, otherwise outputs the color $(0.5, 0.5, 0.5)$.

%\bibliographystyle{plain}
%\bibliography{refs}

\end{document}
